# -*- coding: utf-8 -*-
"""Project 2_Ramis Rawnak

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YTlUfCRc2B8zWoAOzkD7l1QYlB75-9O5
"""

!pip install --upgrade tensorflow

from __future__ import print_function
import pandas as pd
import numpy as np

#Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
#Change the directory to Statistics folder
# %cd /content/drive/My Drive/Statistics/

#Load dataset into a data frame
spam = pd.read_csv("/content/drive/My Drive/Statistics/spam.csv",  sep = ',')
print('Dataset Loaded...')
spam

#fill in the missing feature values with the most popular value/ used values in each column
spam.fillna(spam.mode().iloc[0])
spam

spam_input = spam.drop("Class", axis = 1)

spam_output = spam["Class"]

# Split for training and testing set
from sklearn.model_selection import train_test_split
train_spam_input = spam_input[:1000]
test_spam_input = spam_input[1000:]
train_spam_output = spam_output[:1000]
test_spam_output = spam_output[1000:]

#Load library for classification task
from sklearn.ensemble import RandomForestClassifier
from sklearn import tree
from sklearn.tree import DecisionTreeClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.ensemble import AdaBoostClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report
from sklearn.metrics import accuracy_score
from sklearn.ensemble import VotingClassifier
from sklearn.model_selection import train_test_split

#Decision Tree Classifier
clf_dt = DecisionTreeClassifier()
#Train the Decision Tree Classifier
clf_dt.fit(train_spam_input,train_spam_output)
# Predict the testing
Y_pred_dt = clf_dt.predict(test_spam_input)
# Classification Accuracy of the Decision Tree Classifier
acc_dt = accuracy_score(test_spam_output, Y_pred_dt)
print("Decision tree accuracy: ", acc_dt)
print(confusion_matrix(test_spam_output, Y_pred_dt))

#Gaussian Naive Classifier
clf_gnb = GaussianNB()
#Train the Gaussian Naive Classifier
clf_gnb.fit(train_spam_input,train_spam_output)
# Predict the testing
Y_pred_gnb = clf_gnb.predict(test_spam_input)
# Classification Accuracy of the Gaussian Naive Classifier
acc_gnb = accuracy_score(test_spam_output, Y_pred_gnb)
print("Gaussian Naive accuracy: ", acc_gnb)
print(confusion_matrix(test_spam_output, Y_pred_gnb))

#Logistic Regression
clf_lr = LogisticRegression()
#Train the Logistic Regression
clf_lr.fit(train_spam_input,train_spam_output)
# Predict the testing
Y_pred_lr = clf_lr.predict(test_spam_input)
# Classification Accuracy of the Gaussian Naive Classifier
acc_lr = accuracy_score(test_spam_output, Y_pred_lr)
print("Logistic Regression: ", acc_lr)
print(confusion_matrix(test_spam_output, Y_pred_lr))

# Fuse the classifier with Decision Tree, Gaussian Naive Bayes and Logistic Regression by using majority voting rule
clf_dt_model = DecisionTreeClassifier()
clf_dt_score = clf_dt_model.fit(train_spam_input,train_spam_output)
clf_gnb_model = GaussianNB()
clf_gnb_score = clf_gnb_model.fit(train_spam_input,train_spam_output)
clf_lr_model = LogisticRegression()
clf_lr_score = clf_lr_model.fit(train_spam_input,train_spam_output)
Fusion_model = VotingClassifier(estimators=[('DT', clf_dt_model), ('GNB', clf_gnb_model), ('LR', clf_lr_model)], voting='hard')
Fusion_score = Fusion_model.fit(train_spam_input,train_spam_output)

Y_pred = Fusion_score.predict(test_spam_input)
accuracy_score(test_spam_output, Y_pred)
print("Accuracy:",accuracy_score(test_spam_output, Y_pred))
target_names = ['class 0', 'class 1']
print(classification_report(test_spam_output, Y_pred, target_names=target_names, digits=5))
print(confusion_matrix(test_spam_output, Y_pred))

# AdaBoost Ensemble with Decision Trees as the base learner
abc = AdaBoostClassifier(n_estimators=200, base_estimator=DecisionTreeClassifier())
model = abc.fit(train_spam_input, train_spam_output)
Y_pred_abc = model.predict(test_spam_input)
abc_acc = accuracy_score(test_spam_output, Y_pred_abc)
print("Accuracy of AdaBoost Ensemble with Decision Trees as the base learner: {:.3f}%".format(abc_acc * 100 ))

#Random Forest Classifier
clf_rf = RandomForestClassifier(n_estimators=1000,max_features="auto")
#Train the Random Tree Classifier
clf_rf.fit(train_spam_input, train_spam_output)
# Predict the testing
Y_pred_rf = clf_rf.predict(test_spam_input)
# Classification Accuracy of the Random Forest Classifier
acc_rf = accuracy_score(test_spam_output, Y_pred_rf)
print("Random forest accuracy with 1000 base learners: ", acc_rf)
print(confusion_matrix(test_spam_output, Y_pred_rf))

#Adaboost classifier with Decision Tree as base learner with different training and testing sizes

#Training & Testing splits: 50%-50%
from sklearn.model_selection import train_test_split
X_train, X_test, Y_train, Y_test = train_test_split(spam_input,spam_output, test_size = 0.5, random_state = 0)
#Initialize Decision Tree Classifier and fit into training data
clf_dt = DecisionTreeClassifier()
clf_dt.fit(X_train, Y_train)
#Predict
Y_pred = clf_dt.predict(X_test)
#Accuracy of the classifier
acc_dt= accuracy_score(Y_test, Y_pred)
print("Accuracy:", acc_dt)
#Confusion_Matrix
print(confusion_matrix(Y_test, Y_pred))

# Fuse the classifier with Decision Tree, Gaussian Naive Bayes and Logistic Regression by using majority voting rule
clf_dt_model = DecisionTreeClassifier()
clf_dt_score = clf_dt_model.fit(X_train,Y_train)
clf_gnb_model = GaussianNB()
clf_gnb_score = clf_gnb_model.fit(X_train,Y_train)
clf_lr_model = LogisticRegression()
clf_lr_score = clf_lr_model.fit(X_train,Y_train)
Fusion_model = VotingClassifier(estimators=[('DT', clf_dt_model), ('GNB', clf_gnb_model), ('LR', clf_lr_model)], voting='hard')
Fusion_score = Fusion_model.fit(X_train,Y_train)

Y_pred = Fusion_score.predict(X_test)
accuracy_score(Y_test, Y_pred)
print("Accuracy:",accuracy_score(Y_test, Y_pred))
target_names = ['class 0', 'class 1']
print(classification_report(Y_test, Y_pred, target_names=target_names, digits=5))
print(confusion_matrix(Y_test, Y_pred))

#Training & Testing splits: 60%-40%
from sklearn.model_selection import train_test_split
X_train, X_test, Y_train, Y_test = train_test_split(spam_input,spam_output, test_size = 0.4, random_state = 0)
#Initialize Decision Tree Classifier and fit into training data
clf_dt = DecisionTreeClassifier()
clf_dt.fit(X_train, Y_train)
#Predict
Y_pred = clf_dt.predict(X_test)
#Accuracy of the classifier
acc_dt= accuracy_score(Y_test, Y_pred)
print("Accuracy:", acc_dt)
#Confusion_Matrix
print(confusion_matrix(Y_test, Y_pred))

# Fuse the classifier with Decision Tree, Gaussian Naive Bayes and Logistic Regression by using majority voting rule
clf_dt_model = DecisionTreeClassifier()
clf_dt_score = clf_dt_model.fit(X_train,Y_train)
clf_gnb_model = GaussianNB()
clf_gnb_score = clf_gnb_model.fit(X_train,Y_train)
clf_lr_model = LogisticRegression()
clf_lr_score = clf_lr_model.fit(X_train,Y_train)
Fusion_model = VotingClassifier(estimators=[('DT', clf_dt_model), ('GNB', clf_gnb_model), ('LR', clf_lr_model)], voting='hard')
Fusion_score = Fusion_model.fit(X_train,Y_train)

Y_pred = Fusion_score.predict(X_test)
accuracy_score(Y_test, Y_pred)
print("Accuracy:",accuracy_score(Y_test, Y_pred))
target_names = ['class 0', 'class 1']
print(classification_report(Y_test, Y_pred, target_names=target_names, digits=5))
print(confusion_matrix(Y_test, Y_pred))

#Training & Testing splits: 70%-30%
from sklearn.model_selection import train_test_split
X_train, X_test, Y_train, Y_test = train_test_split(spam_input,spam_output, test_size = 0.3, random_state = 0)
#Initialize Decision Tree Classifier and fit into training data
clf_dt = DecisionTreeClassifier()
clf_dt.fit(X_train, Y_train)
#Predict
Y_pred = clf_dt.predict(X_test)
#Accuracy of the classifier
acc_dt= accuracy_score(Y_test, Y_pred)
print("Accuracy:", acc_dt)
#Confusion_Matrix
print(confusion_matrix(Y_test, Y_pred))

# Fuse the classifier with Decision Tree, Gaussian Naive Bayes and Logistic Regression by using majority voting rule
clf_dt_model = DecisionTreeClassifier()
clf_dt_score = clf_dt_model.fit(X_train,Y_train)
clf_gnb_model = GaussianNB()
clf_gnb_score = clf_gnb_model.fit(X_train,Y_train)
clf_lr_model = LogisticRegression()
clf_lr_score = clf_lr_model.fit(X_train,Y_train)
Fusion_model = VotingClassifier(estimators=[('DT', clf_dt_model), ('GNB', clf_gnb_model), ('LR', clf_lr_model)], voting='hard')
Fusion_score = Fusion_model.fit(X_train,Y_train)

Y_pred = Fusion_score.predict(X_test)
accuracy_score(Y_test, Y_pred)
print("Accuracy:",accuracy_score(Y_test, Y_pred))
target_names = ['class 0', 'class 1']
print(classification_report(Y_test, Y_pred, target_names=target_names, digits=5))
print(confusion_matrix(Y_test, Y_pred))

#Training & Testing splits: 80%-20%
from sklearn.model_selection import train_test_split
X_train, X_test, Y_train, Y_test = train_test_split(spam_input,spam_output, test_size = 0.4, random_state = 0)
#Initialize Decision Tree Classifier and fit into training data
clf_dt = DecisionTreeClassifier()
clf_dt.fit(X_train, Y_train)
#Predict
Y_pred = clf_dt.predict(X_test)
#Accuracy of the classifier
acc_dt= accuracy_score(Y_test, Y_pred)
print("Accuracy:", acc_dt)
#Confusion_Matrix
print(confusion_matrix(Y_test, Y_pred))

# Fuse the classifier with Decision Tree, Gaussian Naive Bayes and Logistic Regression by using majority voting rule
clf_dt_model = DecisionTreeClassifier()
clf_dt_score = clf_dt_model.fit(X_train,Y_train)
clf_gnb_model = GaussianNB()
clf_gnb_score = clf_gnb_model.fit(X_train,Y_train)
clf_lr_model = LogisticRegression()
clf_lr_score = clf_lr_model.fit(X_train,Y_train)
Fusion_model = VotingClassifier(estimators=[('DT', clf_dt_model), ('GNB', clf_gnb_model), ('LR', clf_lr_model)], voting='hard')
Fusion_score = Fusion_model.fit(X_train,Y_train)

Y_pred = Fusion_score.predict(X_test)
accuracy_score(Y_test, Y_pred)
print("Accuracy:",accuracy_score(Y_test, Y_pred))
target_names = ['class 0', 'class 1']
print(classification_report(Y_test, Y_pred, target_names=target_names, digits=5))
print(confusion_matrix(Y_test, Y_pred))